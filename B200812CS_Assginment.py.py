# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1S8Xp_Kq2nG5G1lvnSVdRMQweizxaY05U
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
def f(x):
  return x**3+ 5*(x**2)+7*x+2

x=np.linspace(-4,2,500)
plt.plot(x,f(x),color='red')
plt.show()

store=[]
noise = np.random.normal(0,1,150)
for i in range(150):
  k=random.choice(range(500))
  while k in store:
    k=random.choice(range(500))
  store.append(k)
  x[k]=x[k]+noise[i]

plt.plot(x,f(x),"ro")
plt.show()

X_train, X_test, y_train, y_test = train_test_split(
    x, f(x), test_size=0.2, random_state=0)


def cost_function(x,y,m,b):
    n=len(x)
    y_predicted=m*x+b
    return (1/n)*sum([val**2 for val in (y-y_predicted)])



def gradient_descent(x,y,learning_rate,m_curr,b_curr):


  n=len(x)
  iterations=1000
  
  for i in range(iterations):
    y_predicted=m_curr*x+b_curr
    md=-(2/n)*sum(x*(y-y_predicted))
    bd=-(2/n)*sum(y-y_predicted)
    m_curr=m_curr-learning_rate*md
    b_curr=b_curr-learning_rate*bd
    # print("m {},b {}\n".format(m_curr,b_curr))
  return m_curr,b_curr



m,b= gradient_descent(X_train,y_train,0.001,0,0)
Train_error=cost_function(X_train,y_train,m,b)
print("Train_error is:",Train_error)

Test_error=cost_function(X_test,y_test,m,b)
print("Test_error is:",Test_error)
regression_line=[(m*x)+b for x in X_train ]
plt.scatter(X_train,y_train)
plt.plot(X_train,regression_line)
plt.show()

def cost_function_cubic(x,y,m,b,c,d):
    n=len(x)
    y_predicted=m*(x**3)+b*(x**2)+c*x+d
    return (1/n)*sum([val**2 for val in (y-y_predicted)])

def gradient_descent_cubic(x,y,learning_rate,m_curr,b_curr,c_curr,d_curr):


  n=len(x)
  iterations=1000
  
  for i in range(iterations):
    y_predicted=m_curr*(x**3)+b_curr*(x**2)+c_curr*x+d_curr
   
    md=-(2/n)*sum((x**3)*(y-y_predicted))
    bd=-(2/n)*sum((x**2)*(y-y_predicted))
    cd=-(2/n)*sum(x*(y-y_predicted))
    dd=-(2/n)*sum((y-y_predicted))
    m_curr=m_curr-learning_rate*md
    b_curr=b_curr-learning_rate*bd
    c_curr=c_curr-learning_rate*cd
    d_curr=d_curr-learning_rate*dd
    # print("m {},b {}\n".format(m_curr,b_curr))
  return m_curr,b_curr,c_curr,d_curr






m2,b2,c2,d= gradient_descent_cubic(X_train,y_train,0.001,0,0,0,0)
Train_error_2=cost_function_cubic(X_train,y_train,m2,b2,c2,d)
print("Train_error_2 is:",Train_error_2)
Test_error_2=cost_function_cubic(X_test,y_test,m2,b2,c2,d)
print("Test_error_2 is:",Test_error_2)
regression_line_2=[(m2*(x**3))+b2*(x**2)+c2*x+d for x in X_train ]
plt.scatter(X_train,y_train)
plt.scatter(X_train,regression_line_2)
plt.show()

y=np.empty(500)
for i in range(500):
  if(x[i]<0):
     y[i]=0
  else:
     y[i]=1
arr=[]
p=np.array(arr)
q=np.array(arr)
y1=np.array(arr)
y2=np.array(arr)
for i in range(500):
   if(y[i]==0):
     p=np.append(p,[x[i]])
     y1=np.append(y1,[y[i]])
   if(y[i]==1):
     q=np.append(q,[x[i]])
     y2=np.append(y2,[y[i]])

# print(y1)
# print(p)
# print(q.shape)
plt.scatter(p,y1,color='red',marker='+')
plt.scatter(q,y2,color='blue',marker='+')
plt.show()

X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=0)


def sigmoid(z):
  p=1/(1+np.exp(-1*z)) 
  return p

def gradient_descent(x,y,learning_rate,m_curr,b_curr):


  n=len(x)
  iterations=1000
  
  for i in range(iterations):
    z=m_curr*x+b_curr
    y_predicted=sigmoid(z)
    
    md=-(2/n)*sum(x*(y-y_predicted))
    bd=-(2/n)*sum(y-y_predicted)
    m_curr=m_curr-learning_rate*md
    b_curr=b_curr-learning_rate*bd
    # print("m {},b {}\n".format(m_curr,b_curr))
  return m_curr,b_curr



m2,b2= gradient_descent(X_train,y_train,0.1,0,0)
y_l=m2*X_train+b2
y_p_l=m2*y_test+b2
z=sigmoid(y_l)
y_p=sigmoid(y_p_l)

# print(np.concatenate((y_p.reshape(len(y_p),1), y_test.reshape(len(y_test),1)),1))
print(z.shape)
print(X_train.shape)
plt.scatter(p,y1,color='red',marker='+')
plt.scatter(q,y2,color='blue',marker='+')
plt.scatter(X_train,z,color='green')
x2=-1*b2/m2
print(x2)
plt.axvline(x=x2,color='orange')
plt.show()

from sklearn.metrics import roc_curve,auc
logistic_fpr,logistic_tpr,threshold = roc_curve(y_test,y_p)
auc_logistic = auc(logistic_fpr,logistic_tpr)
plt.figure(figsize=(5,5),dpi=100)
plt.plot(logistic_fpr,logistic_tpr,marker='.',label='Logistic (auc=%0.3f)'%auc_logistic)
plt.plot([0,1],[0,1],'--',color=(0.6,0.6,0.6),label='Random')
plt.xlabel('False Positive Rate -->')
plt.ylabel('True Positive Rate -->')
plt.legend()
plt.show()
print("AUC -",auc_logistic)

from sklearn.metrics import confusion_matrix

for i in range(len(y_p)):
  if(y_p[i]>=0.5):
    y_p[i]=1
  else:
    y_p[i]=0
print(y_p)
cm=confusion_matrix(y_test, y_p)
print(cm)
from sklearn.metrics import accuracy_score
accuracy_score(y_test, y_p)